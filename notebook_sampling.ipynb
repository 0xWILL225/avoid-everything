{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfb09d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set PYTHONPATH environment variable for the kernel\n",
    "robofin_path = os.path.join(os.getcwd(), 'robofin')\n",
    "current_pythonpath = os.environ.get('PYTHONPATH', '')\n",
    "if robofin_path not in current_pythonpath:\n",
    "    os.environ['PYTHONPATH'] = f\"{robofin_path}:{current_pythonpath}\" if current_pythonpath else robofin_path\n",
    "\n",
    "# Also add to sys.path for immediate effect\n",
    "if robofin_path not in sys.path:\n",
    "    sys.path.insert(0, robofin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a73a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        if not obj:\n",
    "            return \"dict[?, ?]\"\n",
    "        key_types = {get_type(k) for k in obj.keys()}\n",
    "        value_types = {get_type(v) for v in obj.values()}\n",
    "        return f\"dict[{', '.join(key_types)}, {', '.join(value_types)}]\"\n",
    "    elif isinstance(obj, list):\n",
    "        if not obj:\n",
    "            return \"list[?]\"\n",
    "        elem_types = {get_type(elem) for elem in obj}\n",
    "        return f\"list[{', '.join(elem_types)}]\"\n",
    "    elif isinstance(obj, tuple):\n",
    "        if not obj:\n",
    "            return \"tuple[?]\"\n",
    "        elem_types = [get_type(elem) for elem in obj]\n",
    "        return f\"tuple[{', '.join(elem_types)}]\"\n",
    "    elif isinstance(obj, set):\n",
    "        if not obj:\n",
    "            return \"set[?]\"\n",
    "        elem_types = {get_type(elem) for elem in obj}\n",
    "        return f\"set[{', '.join(elem_types)}]\"\n",
    "    else:\n",
    "        return type(obj).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ac890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df16f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from robofin.robots import Robot\n",
    "\n",
    "from robofin.robot_constants import FrankaConstants\n",
    "\n",
    "# Load the Robot class with the standard URDF file (that uses relative filepaths)\n",
    "robot = Robot(\"assets/panda/panda.urdf\")\n",
    "# robot = Robot(\"assets/gp7/gp7.urdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ad186",
   "metadata": {},
   "source": [
    "## Sampler testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robofin.samplers import NumpyRobotSampler\n",
    "from robofin.samplers import TorchRobotSampler\n",
    "\n",
    "from robofin.samplers_original import NumpyFrankaSampler\n",
    "from robofin.samplers_original import TorchFrankaSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb0a76",
   "metadata": {},
   "source": [
    "## Compute spheres testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b6616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robofin.samplers_original import TorchFrankaCollisionSampler\n",
    "\n",
    "device = robot.device\n",
    "c_sampler = TorchFrankaCollisionSampler(device)\n",
    "\n",
    "batch_dim = 5\n",
    "torch_neutral_config = torch.tensor(robot.neutral_config, dtype=torch.float32, device=device)\n",
    "batched_configs = torch_neutral_config.unsqueeze(0).repeat(batch_dim, 1).to(device)\n",
    "prismatic_joint = 0.04\n",
    "spheres = c_sampler.compute_spheres(batched_configs, prismatic_joint=prismatic_joint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f1f0749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list[tuple[float, dict[str, ndarray]]]\n",
      "list[tuple[float, dict[str, Tensor]]]\n",
      "list[tuple[float, dict[str, Tensor]]]\n",
      "list[tuple[float, Tensor]]\n",
      "list[tuple[float, Tensor]]\n"
     ]
    }
   ],
   "source": [
    "print(get_type(FrankaConstants.SPHERES))\n",
    "print(get_type(c_sampler.spheres))\n",
    "print(get_type(robot.spheres))\n",
    "print(get_type(spheres))\n",
    "print(get_type(robot.compute_spheres(batched_configs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab121acb",
   "metadata": {},
   "source": [
    "## FK testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c662b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robofin.kinematics.numba import franka_eef_link_fk\n",
    "\n",
    "prismatic_joint = robot.auxiliary_joint_defaults[\"panda_finger_joint1\"]\n",
    "link8_pose = np.eye(4)\n",
    "franka_fk = franka_eef_link_fk(prismatic_joint, base_pose=link8_pose)  # always relative to link8\n",
    "robot_fk = robot.eef_fk(pose=link8_pose, frame=\"panda_link8\")\n",
    "\n",
    "franka_fk_dict = {\n",
    "    \"panda_link8\": franka_fk[0],\n",
    "    \"panda_hand\": franka_fk[1],\n",
    "    \"panda_grasptarget\": franka_fk[2],\n",
    "    \"right_gripper\": franka_fk[3],\n",
    "    \"panda_leftfinger\": franka_fk[4],\n",
    "    \"panda_rightfinger\": franka_fk[5],\n",
    "}\n",
    "\n",
    "print(f\"prismatic_joint = {prismatic_joint}\")\n",
    "\n",
    "for link_name in franka_fk_dict:\n",
    "    if not np.allclose(franka_fk_dict[link_name], robot_fk[link_name]):\n",
    "        print(\"-\"*100)\n",
    "        print(f\"Franka {link_name} pose:\\n{franka_fk_dict[link_name]}\")\n",
    "        print(f\"Robot {link_name} pose:\\n{robot_fk[link_name]}\")\n",
    "        print(f\"Franka {link_name} pose does not match Robot {link_name} pose\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b434ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robofin.robot_constants import FrankaConstants\n",
    "\n",
    "to_link = \"panda_link8\"\n",
    "\n",
    "print(f\"Franka fixed transform {robot.tcp_link_name} -> {to_link}: \\n\", FrankaConstants.EEF_T_LIST[(to_link, robot.tcp_link_name)].inverse.matrix)\n",
    "print(f\"Robot fixed transform {robot.tcp_link_name} -> {to_link}: \\n\", robot.fixed_eef_link_transforms[to_link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aef638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "np_configs: np.ndarray = np.array([robot.neutral_config, robot.neutral_config])\n",
    "torch_configs: torch.Tensor = torch.tensor(np_configs)\n",
    "\n",
    "print(np_configs.shape)\n",
    "print(torch_configs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fde552",
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_np: Dict[str, np.ndarray] = robot.fk(robot.neutral_config)\n",
    "fk_torch: Dict[str, torch.Tensor] = robot.fk_torch(torch.tensor(robot.neutral_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02675ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_visual_fk = robot.visual_fk(np_configs)\n",
    "torch_visual_fk = robot.visual_fk_torch(torch_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bce632",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fk_np.keys())\n",
    "print(fk_torch.keys())\n",
    "fk_torch[\"right_gripper\"]\n",
    "\n",
    "print(np_visual_fk.keys())\n",
    "print(torch_visual_fk.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np_fk_gripper = fk_np[\"right_gripper\"]\n",
    "torch_fk_gripper = fk_torch[\"right_gripper\"].detach().cpu().numpy()\n",
    "\n",
    "print(\"Numpy FK shape:\", np_fk_gripper.shape)\n",
    "print(\"Torch FK shape:\", torch_fk_gripper.shape)\n",
    "print(\"Difference (max abs):\", np.abs(np_fk_gripper - torch_fk_gripper).max())\n",
    "\n",
    "# Optionally, test visual FK as well\n",
    "if hasattr(robot, \"visual_fk\") and hasattr(robot, \"visual_fk_torch\"):\n",
    "    np_visual_fk = robot.visual_fk(np_configs)\n",
    "    torch_visual_fk = robot.visual_fk_torch(torch_configs)\n",
    "    np_vis_gripper = np_visual_fk[\"panda_hand\"]\n",
    "    torch_vis_gripper = torch_visual_fk[\"panda_hand\"].detach().cpu().numpy()\n",
    "    print(\"Visual FK difference (max abs):\", np.abs(np_vis_gripper - torch_vis_gripper).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff272f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_visual_fk = robot.visual_fk(np_configs, link_name=\"panda_hand\")\n",
    "torch_visual_fk = robot.visual_fk_torch(torch_configs, link_name=\"panda_hand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from robofin.samplers import NumpyRobotSampler\n",
    "from robofin.samplers_original import NumpyFrankaSampler\n",
    "\n",
    "franka_visual_fk = franka_arm_visual_fk(robot.neutral_config, 0.02, np.eye(4))\n",
    "\n",
    "sampler = NumpyRobotSampler(robot=robot, num_robot_points=1000, num_eef_points=100, use_cache=True)\n",
    "sampler.sample(robot.neutral_config, 0.02, num_points=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae8f8b",
   "metadata": {},
   "source": [
    "## Dataloader testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avoid_everything.data_loader import DataModule\n",
    "\n",
    "data_module_parameters = {\n",
    "    \"data_dir\": \"/workspace/datasets/ae_aristotle1_5mm_cubbies\",\n",
    "    \"train_trajectory_key\": \"global_solutions\",\n",
    "    \"val_trajectory_key\": \"global_solutions\",\n",
    "    \"num_obstacle_points\": 4096,\n",
    "    \"random_scale\": 0.015\n",
    "}\n",
    "shared_parameters = {\n",
    "    \"prismatic_joint\": 0.04,\n",
    "    \"num_robot_points\": 2048,\n",
    "    \"num_target_points\": 128,\n",
    "    \"action_chunk_length\": 1\n",
    "}\n",
    "cfg = {\n",
    "    \"train_batch_size\": 10,\n",
    "    \"val_batch_size\": 10,\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "\n",
    "dm = DataModule(\n",
    "    train_batch_size=cfg[\"train_batch_size\"],\n",
    "    val_batch_size=cfg[\"val_batch_size\"],\n",
    "    num_workers=cfg[\"num_workers\"],\n",
    "    **data_module_parameters,\n",
    "    **shared_parameters,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "dl = dm.train_dataloader()\n",
    "sample = dl.dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def convert_to_numpy_f32(arr: np.ndarray | torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a NumPy array or Torch tensor to a NumPy float32 array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray or torch.Tensor\n",
    "        Input array to convert.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Converted array with dtype float32.\n",
    "    \"\"\"\n",
    "    if isinstance(arr, torch.Tensor):\n",
    "        np_arr: np.ndarray = arr.cpu().numpy()\n",
    "    elif isinstance(arr, np.ndarray):\n",
    "        np_arr: np.ndarray = arr\n",
    "    else:\n",
    "        raise TypeError(\"convert_to_numpy_f32: Input must be a NumPy array or Torch tensor\")\n",
    "    return np_arr.astype(np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuboid_dims = convert_to_numpy_f32(sample[\"cuboid_dims\"])\n",
    "cuboid_centers = convert_to_numpy_f32(sample[\"cuboid_centers\"])\n",
    "cuboid_quaternions = convert_to_numpy_f32(sample[\"cuboid_quats\"])\n",
    "for dims, center, quat in zip(cuboid_dims, cuboid_centers, cuboid_quaternions):\n",
    "    print(f\"Lengths: Dimensions: {len(dims)}, Center: {len(center)}, Quaternion: {len(quat)}\")\n",
    "\n",
    "cylinder_radii = convert_to_numpy_f32(sample[\"cylinder_radii\"])\n",
    "cylinder_heights = convert_to_numpy_f32(sample[\"cylinder_heights\"])\n",
    "cylinder_centers = convert_to_numpy_f32(sample[\"cylinder_centers\"])\n",
    "cylinder_quaternions = convert_to_numpy_f32(sample[\"cylinder_quats\"])\n",
    "for radius, height, center, quat in zip(cylinder_radii, cylinder_heights, cylinder_centers, cylinder_quaternions):\n",
    "    print(f\"Lengths: Radius: {len(radius)}, Height: {len(height)}, Center: {len(center)}, Quaternion: {len(quat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import viz_client\n",
    "\n",
    "urdf_path = \"/workspace/assets/panda/panda_collision_spheres_gltf.urdf\"\n",
    "if not os.path.exists(urdf_path):\n",
    "    print(f\"‚ùå URDF not found at {urdf_path}\")\n",
    "    print(\"Please update the urdf_path variable in test_connect()\")\n",
    "    raise FileNotFoundError(f\"URDF not found at {urdf_path}\")\n",
    "\n",
    "viz_client.connect(urdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d165a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_client.publish_obstacles(cuboid_centers=cuboid_centers,\n",
    "                            cuboid_dims=cuboid_dims,\n",
    "                            cuboid_quaternions=cuboid_quaternions,\n",
    "                            cylinder_centers=cylinder_centers,\n",
    "                            cylinder_radii=cylinder_radii,\n",
    "                            cylinder_heights=cylinder_heights,\n",
    "                            cylinder_quaternions=cylinder_quaternions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_client.clear_obstacles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3271b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_client.publish_joints(joints=robot.neutral_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3905d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
